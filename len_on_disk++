        h.lpa = 0;

again:
        lpa_t lpa = get_lpa(cache, key, &h);
        struct cmt_struct *cmt = c->cmt[IDX(lpa)];
        //if(cmt->t_ppa != UINT_MAX) {
        //    NVMEV_INFO("Trying CMT %lu with PPA %u cnt %u\n", 
        //            IDX(lpa), c->cmt[IDX(lpa)]->t_ppa, h.cnt);
        //}

        if(cmt->t_ppa == UINT_MAX) {
            continue;
        }

        cmt->state = DIRTY;

        //NVMEV_ASSERT(cmt->pt);
        if(!cmt->pt) {
            continue;
        }

        if(cmt->pt[OFFSET(lpa)].lpa == lpa) {
            h.cnt++;
            collision++;
            goto again;
        }

        //if(at != i) {
        //    NVMEV_ERROR("Wanted %llu at %llu had %llu\n", i, ((i * GRAINED_UNIT)), at);
        //}

        cmt->pt[OFFSET(lpa)].lpa = lpa;
        cmt->pt[OFFSET(lpa)].ppa = i;
        cmt->cached_cnt++;

        if((cmt->cached_cnt * ENTRY_SIZE) % GRAINED_UNIT == 0) {
            cmt->len_on_disk++;
        }

        if ((i & 1048575) == 0) {
            NVMEV_INFO("Finished %llu out of %llu grains\n", i, size / GRAINED_UNIT);
        }
--
    uint32_t **oob = s->oob;

    uint32_t cnt = 0;
    for(int i = 0; i < cache->env.nr_valid_tpages; i++) {
        struct cmt_struct *cmt = c->cmt[i];
        if(cmt->pt) {
            for(int j = 0; j < EPP; j++) {
                if(cmt->pt[j].lpa != UINT_MAX) {
                    lpas[cnt] = cmt->pt[j].lpa;
                    ppas[cnt] = cmt->pt[j].ppa;
                    cnt++;
                    //cmt->pt[i].lpa = cmt->pt[i + 1].lpa;
                    //cmt->pt[i].ppa = cmt->pt[i + 1].ppa;

                    //NVMEV_INFO("Moving %u %u\n", cmt->pt[j].lpa, cmt->pt[j].ppa);
                }
            }

            if(cnt > 0) {
                if(cnt != cmt->cached_cnt) {
                    NVMEV_INFO("cnt was %u cached cnt %u\n", cnt, cmt->cached_cnt);
                }

                NVMEV_ASSERT(cnt == cmt->cached_cnt);
                for(int i = 0; i < cnt; i++) {
                    cmt->pt[i].lpa = lpas[i];
                    cmt->pt[i].ppa = ppas[i];
                }
            }

            cnt = 0;

            mark_grain_invalid(s, PPA_TO_PGA(cmt->t_ppa, cmt->len_on_disk), 
                               GRAIN_PER_PAGE - cmt->len_on_disk);
            for(int i = 1; i < cmt->len_on_disk; i++) {
                oob[cmt->t_ppa][i] = 0;
            }
            for(int i = cmt->len_on_disk; i < GRAIN_PER_PAGE; i++) {
                oob[cmt->t_ppa][i] = UINT_MAX;
            }
            cmt->pt = NULL;
        }
    }

    fastmode = false;
    NVMEV_INFO("Fast fill done. %llu collisions\n", collision);
--
				   g_len++;
				}
#endif

#ifdef GC_STANDARD
                NVMEV_ASSERT(cmt->g_off == 0 && i == 0);
#endif
                if(cmt->t_ppa != pgidx || cmt->g_off != i) {
                    NVMEV_DEBUG("CMT IDX %u moved from PPA %llu to PPA %u\n", 
                                 idx, pgidx, cmt->t_ppa);
                    continue;
                }

                NVMEV_DEBUG("CMT IDX %u was %u grains in size from grain %llu PPA %llu (%u)\n", 
                             idx, g_len, grain, G_IDX(grain + i), cmt->t_ppa);
                mark_grain_invalid(shard, grain, g_len);

                lpa_lens[lpa_len_idx++] = (struct lpa_len_ppa) {idx, g_len, 
                                           grain, UINT_MAX - 1, 
                                           UINT_MAX};
--
void __pte_evict_work(void *voidargs, uint64_t*, uint64_t*) {
    struct pte_e_args *args = (struct pte_e_args*) voidargs;

    uint64_t out_ppa = args->out_ppa;
    struct cmt_struct *cmt = args->cmt;
    struct pt_struct *pt = cmt->pt;
    uint64_t idx = args->idx;
    struct ssdparams *spp = args->spp;
    uint64_t start_lpa = idx * EPP;
    uint32_t grain = args->grain;
    uint32_t len = args->len * GRAINED_UNIT;
--

#ifndef GC_STANDARD
    extra = sizeof(lpa_t);
#endif

    //NVMEV_INFO("In %s %p\n", __func__, &cmt->outgoing);

    /*
     * TODO shard off.
     */

--
        //if(ppa != UINT_MAX) {
        //    NVMEV_INFO("Sending out LPA %llu PPA %u in %s.\n", start_lpa + i, ppa, __func__);
        //}
    }

    cmt->lru_ptr = NULL;
    cmt->pt = NULL;

    atomic_dec(&cmt->outgoing);
    kfree(args);

    NVMEV_DEBUG("%s done for IDX %llu new ppa %llu old PPA %llu\n", 
                 __func__, idx, out_ppa, args->prev_ppa);
}
--

    spp = &shard->ssd->sp;
#ifdef GC_STANDARD
    return per_page; // spp->pgsz / GRAINED_UNIT;
#else
    cnt = cmt->cached_cnt;
    ret = (cnt * ENTRY_SIZE) / GRAINED_UNIT;

    if((cnt * ENTRY_SIZE) % GRAINED_UNIT) {
        ret++;
    }

    return ret > cmt->len_on_disk ? ret : cmt->len_on_disk;
#endif
}

#ifndef GC_STANDARD
struct expand_w_args {
--
    len = args->len;

    memcpy(new, old, len);

    for(int i = len / ENTRY_SIZE; i < spp->pgsz / ENTRY_SIZE; i++) {
        cmt->pt[i].lpa = UINT_MAX;
        cmt->pt[i].ppa = UINT_MAX;
    }

    cmt->pt[cmt->cached_cnt].lpa = pte.lpa;
    cmt->pt[cmt->cached_cnt].ppa = pte.ppa;
    cmt->cached_cnt++;

    cmt->state = DIRTY;
    //lru_update(cmbr->lru, cmt->lru_ptr);

    atomic_dec(&cmt->outgoing);
    kfree(args);
}

static inline uint32_t __need_expand(struct cmt_struct *cmt) {
    uint32_t expand_to = (cmt->cached_cnt * ENTRY_SIZE) / GRAINED_UNIT;

    if((cmt->cached_cnt * ENTRY_SIZE) % GRAINED_UNIT) {
        expand_to++;
    }

    if(expand_to < cmt->len_on_disk) {
        return false;
    }

    return (cmt->cached_cnt * ENTRY_SIZE) % GRAINED_UNIT == 0;
}

/*
 * This function just assigns a new physical page
 * to the mapping entry. Some space is wasted if the mapping
--
    cache = shard->cache;
    spp = &shard->ssd->sp;
    cmbr = &cache->member;
    oob = shard->oob;

    mark_grain_invalid(shard, PPA_TO_PGA(cmt->t_ppa, cmt->g_off), 
                       cmt->len_on_disk);

    /*
     * Mapping table entries can't be over a page in size.
     */

    NVMEV_ASSERT(cmt->len_on_disk < GRAIN_PER_PAGE);

skip:
    p = get_new_page(shard, MAP_IO);
    ppa = ppa2pgidx(shard, &p);

    BUG_ON(!cmt->lru_ptr);

    advance_write_pointer(shard, MAP_IO);
    mark_page_valid(shard, &p);
    mark_grain_valid(shard, PPA_TO_PGA(ppa, 0), GRAIN_PER_PAGE);

--
    uint64_t shard_off = shard->id * spp->tt_pgs * spp->pgsz;
    uint64_t off = shard_off + ((uint64_t) ppa * spp->pgsz);
    uint8_t *ptr = nvmev_vdev->ns[0].mapped + off;

    NVMEV_DEBUG("Expanded IDX %u from PPA %u grain %llu to PPA %llu new len %u %u cached\n", 
                 cmt->idx, cmt->t_ppa, cmt->g_off, ppa, cmt->len_on_disk + 1,
                 cmbr->nr_cached_tentries);

    struct pt_struct *old = cmt->pt;
    cmt->pt = (struct pt_struct*) ptr;

    uint32_t g_off = cmt->len_on_disk * GRAINED_UNIT;

    struct expand_w_args *args;

    args = (struct expand_w_args*) kmalloc(sizeof(*args), GFP_KERNEL);
    args->shard = shard;
    args->spp = spp;
    args->cmt = cmt;
    args->old = old;
    args->new = cmt->pt;
    args->pte = pte;
    args->cmbr = cmbr;
    args->len = g_off;

    //NVMEV_ASSERT(atomic_read(&cmt->outgoing) == 0);
    atomic_inc(&cmt->outgoing);
    schedule_internal_operation_cb(UINT_MAX, 0,
            NULL, 0, 0, 
            (void*) __expand_work, 
            (void*) args, 
            false, NULL);

    cmt->t_ppa = ppa;
    cmt->g_off = 0;
    cmt->len_on_disk++;
    cmbr->nr_cached_tentries++;

    oob[ppa][0] = cmt->idx * EPP;
    for(int i = 1; i < cmt->len_on_disk; i++) {
        oob[ppa][i] = 0;
    }

    for(int i = cmt->len_on_disk; i < GRAIN_PER_PAGE; i++) {
        oob[ppa][i] = UINT_MAX;
    }

    if(GRAIN_PER_PAGE - cmt->len_on_disk > 0) {
        mark_grain_invalid(shard, PPA_TO_PGA(ppa, cmt->len_on_disk), 
                           GRAIN_PER_PAGE - cmt->len_on_disk);
    }
}
#endif

static void __update_map(struct demand_shard *shard, struct cmt_struct *cmt,
                         lpa_t lpa, struct pt_struct pte, uint32_t pos) {
    struct demand_cache *cache = shard->cache;
    struct cache_member *cmbr = &cache->member;

    NVMEV_ASSERT(cmt->pt);
#ifdef GC_STANDARD
    cmt->pt[OFFSET(lpa)] = pte;
    cmt->state = DIRTY;
    //lru_update(cmbr->lru, cmt->lru_ptr);
#else
    //NVMEV_INFO("LPA %u gets PPA %u\n", lpa, pte.ppa);
    cmt->state = DIRTY;
    if(pos != UINT_MAX) {
        if(cmt->pt[pos].lpa == lpa && cmt->pt[pos].ppa != UINT_MAX) {
            /*
             * The PPA can be UINT_MAX here if it was previously
             * deleted.
             */
            __record_inv_mapping(shard, lpa, cmt->pt[pos].ppa, NULL);
        }

        //NVMEV_INFO("Had Pos %u for LPA %u it was LPA %u\n", pos, lpa,
        //            cmt->pt[pos].lpa);
        NVMEV_ASSERT(cmt->pt[pos].lpa == lpa);
        cmt->pt[pos] = pte;
        return;
	} else if(cmt->cached_cnt == 0) {
		cmt->pt[0].lpa = lpa;
		cmt->pt[0].ppa = pte.ppa;
        cmt->cached_cnt++;
        //NVMEV_INFO("LPA %u added to pos %u\n", lpa, 0);
		return;
    } else if(!__need_expand(cmt)) {
        //NVMEV_INFO("LPA %u added to pos %u\n", lpa, cmt->cached_cnt);
        cmt->pt[cmt->cached_cnt++] = pte;
        return;
    }

	if(cmt->cached_cnt > 0 && __need_expand(cmt)) {
		__expand_map_entry(shard, cmt, pte);
	} else {
		NVMEV_ASSERT(false);
		cmt->pt[cmt->cached_cnt].lpa = lpa;
		cmt->pt[cmt->cached_cnt].ppa = pte.ppa;
		cmt->cached_cnt++;
	}
#endif
}

static bool __cache_hit(struct demand_cache *cache, lpa_t lpa) {
    struct cache_member *cmbr = &cache->member;
    struct cmt_struct *cmt = cache->get_cmt(cache, lpa);
    return cmt->pt != NULL;
}

uint32_t lpas[1024] __attribute__((aligned(32)));
struct pt_struct __lpa_to_pte(struct demand_shard *shard,
                              struct cmt_struct *cmt, lpa_t lpa,
                              uint32_t *pos) {
#ifdef GC_STANDARD
    return cmt->pt[OFFSET(lpa)];
#else
    uint32_t g_len = __entries_to_grains(shard, cmt);
    struct pt_struct pte;
    struct pt_struct *found_pte;

--
	//size_t count = ((g_len * GRAINED_UNIT) / ENTRY_SIZE) * 2;
    //if(count < 16) {
    //    count = 16;
    //}
	////for(int i = 0; i < count; i++) {
    ////    lpas[i] = cmt->pt[i].lpa;
    ////}

    //uint64_t start = get_cycles();
	//////NVMEV_INFO("Trying %lu items\n", (g_len * GRAINED_UNIT) / ENTRY_SIZE);

	//for(int i = 0; i < cmt->cached_cnt; i++) {
    //    if(cmt->pt[i].lpa != UINT_MAX) {
    //        NVMEV_INFO("Inside : %u\n", cmt->pt[i].lpa);
    //    }
	//}

    ////NVMEV_ASSERT(count % 8 == 0);

	//int idx = find(cmt->pt, lpa, count);
    //uint64_t end = get_cycles();

    //if(idx >= 0) {
	//	int orig = idx;
    //    idx /= 2;
    //    NVMEV_INFO("Found LPA %u at IDX %d orig %d took %llu cycles\n", 
    //                lpa, idx, orig, end - start);
    //    
    //    //if(cmt->pt[idx].lpa != lpa) {
    //    //    NVMEV_INFO("Cached cnt %u\n", cmt->cached_cnt);
    //    //}

    //    //if(cmt->pt[idx].lpa != lpa) {
    //    //    NVMEV_INFO("LPA was actually %u\n", cmt->pt[idx].lpa);
    //    //}

    //    NVMEV_ASSERT(cmt->pt[idx].lpa == lpa);
    //    pte.ppa = cmt->pt[idx].ppa;
    //    *pos = idx;
    //} else {
    //    NVMEV_INFO("CMT IDX %u idx %d couldn't find LPA %u took %llu cycles count %lu\n", 
    //                cmt->idx, idx, lpa, end - start, count);
    //    for(int i = 0; i < cmt->cached_cnt; i++) {
    //        if(cmt->pt[i].lpa != UINT_MAX) {
    //            NVMEV_INFO("2 Inside : %u %u\n", cmt->pt[i].lpa, cmt->pt[i].ppa);
    //        }
    //    }

    //    *pos = UINT_MAX;
    //}
--
    //    return *found_pte;
    //} else {
    //    return pte;
    //}

	//size_t idx = find_index(lpa, cmt->pt, (g_len * GRAINED_UNIT) / ENTRY_SIZE);
	//if(idx != UINT_MAX) {
    //    pte.lpa = lpa;
	//	pte.ppa = cmt->pt[idx].ppa;
	//	*pos = idx;
	//} else {
    //    *pos = UINT_MAX;
    //}
    for(int i = 0; i < cmt->cached_cnt; i++) {
        if(cmt->pt[i].lpa == lpa) {
            pte.lpa = lpa;
            pte.ppa = cmt->pt[i].ppa;

            if(pos) {
                *pos = i;
            }

--

    cache = shard->cache;
    cmbr = &cache->member;
    spp = &shard->ssd->sp;
    cstat = &cache->stat;
    grain = cmt->g_off;
	oob = shard->oob;

	if(!fastmode) {
		while(atomic_read(&cmt->outgoing) == 1) {
			cpu_relax();
		}
	}

    uint64_t off = ((uint64_t) cmt->t_ppa * spp->pgsz) + (grain * GRAINED_UNIT);
    uint8_t *ptr = nvmev_vdev->ns[0].mapped + off;
    cmt->pt = (struct pt_struct*) ptr;

    //NVMEV_INFO("__get_one for IDX %u CMT PPA %u\n", cmt->idx, cmt->t_ppa);

    if(!first) {
        /*
         * If this wasn't the first access of this mapping
         * table page, we need to read it from disk.
         */

        if(cmt->t_ppa > spp->tt_pgs) {
            NVMEV_INFO("%s tried to convert PPA %u\n", __func__, cmt->t_ppa);
        }

        if(!fastmode) {
            struct ppa p = ppa_to_struct(spp, cmt->t_ppa);
            struct nand_cmd srd = {
                .type = USER_IO,
                .cmd = NAND_READ,
                .stime = __stime_or_clock(stime),
                .interleave_pci_dma = false,
--
            };

            nsecs_completed = ssd_advance_nand(shard->ssd, &srd);
            d_stat.trans_r += spp->pgsz;
        }
        //NVMEV_INFO("Sent a read for CMT PPA %u\n", cmt->t_ppa);
    } else {
#ifdef GC_STANDARD
        cmt->len_on_disk = GRAIN_PER_PAGE;
        cmt->g_off = 0;
        
        for(int i = 0; i < spp->pgsz / ENTRY_SIZE; i++) {
            cmt->pt[i].ppa = UINT_MAX;
        }
#else
        cmt->len_on_disk = ORIG_GLEN;
        cmt->g_off = 0;

        for(int i = 0; i < spp->pgsz / ENTRY_SIZE; i++) {
            cmt->pt[i].lpa = UINT_MAX;
            cmt->pt[i].ppa = UINT_MAX;
        }
#endif
    }

	cmt->lru_ptr = my_queue_enqueue(q, (void*) cmt);
    //cmt->lru_ptr = lru_push(cmbr->lru, (void *)cmt);
    cmbr->nr_cached_tentries += cmt->len_on_disk;

    *missed = true;
    cstat->cache_miss++;

#ifndef GC_STANDARD
    if(!first && !cgo_is_full(cache)) {
        /*
         * Also bring in the remaining mapping entries on the page.
         */
        NVMEV_ASSERT(oob[cmt->t_ppa][0] != 2);
        struct cmt_struct *found_cmt;

        int brought = 0;
        for(int i = 0; i < GRAIN_PER_PAGE; i++) {
            uint32_t lpa_at_oob = oob[cmt->t_ppa][i];
            uint32_t idx_at_oob = IDX(lpa_at_oob);

			/*
			 * TODO must be a cleaner way to go about this.
			 */

			if(lpa_at_oob != UINT_MAX && lpa_at_oob != 0 && lpa_at_oob != 2 && 
					idx_at_oob != cmt->idx) {
				//NVMEV_INFO("Trying to bring in IDX %u LPA %u\n", 
				//		idx_at_oob, lpa_at_oob);
				found_cmt = cache->get_cmt(cache, IDX2LPA(idx_at_oob));

				if(found_cmt->t_ppa == cmt->t_ppa) {
					if(!found_cmt->pt) {
						//NVMEV_INFO("Bringing in CMT IDX %u PPA %u grain %u\n", 
						//        found_cmt->idx, found_cmt->t_ppa, i);
						brought++;

						if(!fastmode) {
							while(atomic_read(&found_cmt->outgoing) == 1) {
								cpu_relax();
							}
						}

						NVMEV_ASSERT(found_cmt->t_ppa != UINT_MAX);
						NVMEV_ASSERT(!found_cmt->pt);
						NVMEV_ASSERT(!found_cmt->lru_ptr);

						off = ((uint64_t) found_cmt->t_ppa * spp->pgsz) + (i * GRAINED_UNIT);
						ptr = nvmev_vdev->ns[0].mapped + off;
						found_cmt->pt = (struct pt_struct*) ptr;
						//found_cmt->lru_ptr = lru_push(cmbr->lru, (void *)found_cmt);
						found_cmt->lru_ptr = my_queue_enqueue(q, (void*) found_cmt);

						cmbr->nr_cached_tentries += found_cmt->len_on_disk;
						//NVMEV_ASSERT(cmbr->nr_cached_tentries == cmbr->lru->size);
						//NVMEV_INFO("Increased cached tentries to %u\n", 
						//        cmbr->nr_cached_tentries);
					}
				}

				i += cmt->len_on_disk - 1;
			}

            if(cgo_is_full(cache)) {
                break;
            }
--

        status = KV_ERR_KEY_NOT_EXIST;
        goto out;
    }

    if(cmt->t_ppa == UINT_MAX) {
        //NVMEV_INFO("Key %s (%llu) tried to read missing CMT entry.\n", 
        //        (char*) cmd->kv_store.key, *(uint64_t*) cmd->kv_store.key);
        h.cnt++;

        missed = false;
        goto lpa;
    }

    while(atomic_read(&cmt->outgoing) == 1) {
        cpu_relax();
    }

cache:
    if(__cache_hit(cache, lpa)) { 
--
        if(!missed) {
            cstat->cache_hit++;
        }

        goto out;
    } else if(cmt->t_ppa != UINT_MAX) {
        if (cgo_is_full(cache)) {
            nsecs_completed = __evict_one(shard, req, nsecs_latest, &credits);
            nsecs_latest = max(nsecs_latest, nsecs_completed);
            d_stat.t_write_on_read += spp->pgsz;
        }
--
    struct cmt_struct *cmt = cache->get_cmt(cache, lpa);
    struct cache_stat *cstat = &cache->stat;

    //NVMEV_INFO("Got LPA %u IDX %lu\n", lpa, IDX(lpa));

    if(cmt->t_ppa == UINT_MAX) {
        /*
         * Previously unused cached mapping table entry.
         * Different from the original implementation, we
         * actually give it a page here when we first see it,
         * so we have an area of virt's reserved memory to
--
        /*
         * The IDX is used during GC so that we know which CMT entry
         * to update.
         */

        oob[ppa][0] = cmt->idx * EPP;

        cmt->t_ppa = ppa;
        if(!fastmode) {
            cmt->pt = NULL;
        } else {
            cmt->pt = (struct pt_struct*) (nvmev_vdev->ns[0].mapped + 
                                          ((uint64_t) ppa) * spp->pgsz);
        }

        /*
         * Despite the new scheme only using as many mapping entries as needed,
--
         * set its length to the size of a page for now and waste some space
         * until its evicted.
         */

#ifdef GC_STANDARD
        cmt->len_on_disk = GRAIN_PER_PAGE;
        for(int i = 1; i < GRAIN_PER_PAGE; i++) {
            oob[ppa][i] = 0;
        }
#else
        //NVMEV_INFO("CMT remaining\n");
--
            if(ORIG_GLEN < GRAIN_PER_PAGE) {
                mark_grain_invalid(shard, PPA_TO_PGA(ppa, ORIG_GLEN), 
                        GRAIN_PER_PAGE - ORIG_GLEN);
            }

            cmt->len_on_disk = ORIG_GLEN;

            for(int i = 1; i < ORIG_GLEN; i++) {
                oob[ppa][i] = 0;
            }

--
                 * checked.
                 */
                oob[ppa][i] = UINT_MAX;
            }
        } else {
            cmt->len_on_disk = 1;
            cmt->g_off = 0;

            for(int i = 0; i < EPP; i++) {
                cmt->pt[i].lpa = UINT_MAX;
                cmt->pt[i].ppa = UINT_MAX;
            }
        }
#endif

#ifndef GC_STANDARD
        cmt->cached_cnt = 0;
#endif
        /*
         * Will be marked dirty below in update().
         */

        cmt->state = CLEAN;
        first = true;
        //NVMEV_INFO("Assigned a new T PPA %u to IDX %u\n", ppa, cmt->idx);
	}

	if(!fastmode) {
		while(atomic_read(&cmt->outgoing) == 1) {
			cpu_relax();
		}
	}

    if(fastmode) {
--
        if(!missed) {
            cstat->cache_hit++;
        }

        shard->cache->touch(shard->cache, lpa);
    } else if(cmt->t_ppa != UINT_MAX) {
        //NVMEV_INFO("Miss for LPA %u IDX %lu\n", lpa, IDX(lpa));

        if (cgo_is_full(cache)) {
            nsecs_completed = __evict_one(shard, req, nsecs_latest, &credits);
            nsecs_latest = max(nsecs_latest, nsecs_completed);
--
    consume_write_credit(shard, credits);
    check_and_refill_write_credit(shard);
    nsecs_latest = max(nsecs_latest, nsecs_completed);

    if(!fastmode) {
		atomic_inc(&cmt->outgoing);
        ret->cb = __release_map;
        ret->args = &cmt->outgoing;
        ret->nsecs_target = nsecs_latest;
    } else {
fm_out:

        //NVMEV_INFO("%s for key %llu (%llu) klen %u vlen %u grain %llu PPA %llu LPA %u\n", 
--
        args.from = (void*) cmd->kv_store.dptr.prp1;
        args.to = cmd->kv_store.rsvd;
        args.g_len = glen;
        args.vlen = vlen;
        args.klen = klen;
        //args.cmt_outgoing = &cmt->outgoing;

        uint64_t at = *(uint64_t*) (((char*) args.from)  + 1);

        memcpy(nvmev_vdev->ns[0].mapped + args.to, args.from, args.g_len * GRAINED_UNIT);
        memmove(nvmev_vdev->ns[0].mapped + args.to + 
